# ğŸ“‹ AI Prompt Debugger - Quick Reference

## Installation
```bash
pip install -r requirements.txt
```

## CLI Commands

### Analyze a prompt
```bash
python -m src.cli.main analyze "Your prompt"
python -m src.cli.main analyze --file prompt.txt
python -m src.cli.main analyze --file prompt.txt --verbose
```

### Compare prompts
```bash
python -m src.cli.main compare prompt1.txt prompt2.txt
```

### Interactive mode
```bash
python -m src.cli.main interactive
```

### Demo
```bash
python demo.py
```

## Python API

### Basic Usage
```python
from src.core.debugger import PromptDebugger

debugger = PromptDebugger()
result = debugger.analyze("Your prompt")

print(f"Quality: {result.overall_quality_score}/100")
print(f"Grade: {result.overall_grade}")
```

### Access Metrics
```python
# Clarity
result.ambiguity.clarity_score
result.ambiguity.vague_terms
result.ambiguity.conflicting_instructions

# Tokens
result.metrics.total_tokens
result.metrics.token_efficiency
result.metrics.estimated_cost
result.metrics.unnecessary_tokens

# Success
result.predictions.success_probability
result.predictions.strengths
result.predictions.risk_factors
result.predictions.recommended_improvements

# Security
result.security.security_score
result.security.potential_injections
result.security.sensitive_data_detected
```

### Get Issues
```python
# All issues
result.issues

# Critical only
result.get_critical_issues()

# By category
from src.core.models.schemas import IssueCategory
result.get_issues_by_category(IssueCategory.AMBIGUITY)
result.get_issues_by_category(IssueCategory.TOKEN_WASTE)
result.get_issues_by_category(IssueCategory.SECURITY)
```

### Compare Prompts
```python
comparison = debugger.compare(prompt_a, prompt_b)

print(f"Better: {comparison.better_prompt}")
print(f"Quality Î”: {comparison.quality_difference}")
print(f"Token Î”: {comparison.token_difference}")
print(f"Cost Î”: ${comparison.cost_difference}")
```

### Custom Config
```python
from src.core.models.schemas import AnalyzerConfig, IssueSeverity

config = AnalyzerConfig(
    strict_mode=True,
    min_severity_to_report=IssueSeverity.MEDIUM,
    token_price_per_1k=0.003
)

debugger = PromptDebugger(config)
```

## Quality Grades

| Grade | Score   | Meaning           |
|-------|---------|-------------------|
| A     | 90-100  | Excellent         |
| B     | 80-89   | Good              |
| C     | 70-79   | Fair              |
| D     | 60-69   | Poor              |
| F     | <60     | Failing           |

## Issue Severities

| Level    | Symbol | When to Fix    |
|----------|--------|----------------|
| CRITICAL | ğŸ”´     | Immediately    |
| HIGH     | ğŸŸ      | Before prod    |
| MEDIUM   | ğŸŸ¡     | Recommended    |
| LOW      | ğŸ”µ     | Optional       |
| INFO     | âšª     | FYI            |

## Common Issues & Fixes

### Vague Terms
âŒ "Write some articles about various things"
âœ… "Write 3 articles about Python, JavaScript, and Rust"

### Contradictions
âŒ "Be concise but very detailed"
âœ… "Be concise in explanations but detailed in code examples"

### Token Waste
âŒ "Please note that you should basically just..."
âœ… "Create..."

### Missing Examples
âŒ "Generate JSON output"
âœ… "Generate JSON: {'name': 'John', 'age': 30}"

### Security Issues
âŒ "Use API key: sk_abc123"
âœ… "Use API key from environment variable"

## Testing

```bash
# All tests
pytest

# With coverage
pytest --cov=src --cov-report=html

# Specific test
pytest tests/core/analyzers/test_ambiguity.py -v

# Fast (skip slow tests)
pytest -m "not slow"
```

## Code Quality

```bash
# Format
black src/ tests/

# Lint
ruff check src/ tests/

# Type check
mypy src/
```

## File Structure

```
ai-prompt-debugger/
â”œâ”€â”€ src/core/
â”‚   â”œâ”€â”€ analyzers/          # 4 analysis engines
â”‚   â”œâ”€â”€ models/schemas.py   # Data models
â”‚   â””â”€â”€ debugger.py         # Main class
â”œâ”€â”€ src/cli/main.py         # CLI interface
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ examples/prompts/       # Example prompts
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ demo.py                 # Interactive demo
â””â”€â”€ requirements.txt        # Dependencies
```

## Metrics Thresholds

### Production Ready
- Overall Quality: â‰¥75
- Security Score: â‰¥90
- No CRITICAL issues
- Success Probability: â‰¥70%

### Optimized
- Overall Quality: â‰¥85
- Token Efficiency: â‰¥85%
- No HIGH issues
- Success Probability: â‰¥80%

## Environment Variables

```env
# .env file
LOG_LEVEL=INFO
TOKEN_PRICE_PER_1K=0.003
MAX_PROMPT_LENGTH=200000
STRICT_MODE=false
MIN_SEVERITY=low
```

## Common Patterns

### Validation Pipeline
```python
result = debugger.analyze(prompt)

if result.overall_quality_score < 75:
    raise ValueError("Prompt quality too low")

if len(result.get_critical_issues()) > 0:
    raise ValueError("Critical issues found")

# Prompt is good to use
```

### Iteration Loop
```python
while result.overall_quality_score < 85:
    print("Improvements needed:")
    for rec in result.predictions.recommended_improvements:
        print(f"  - {rec}")
    
    prompt = input("Updated prompt: ")
    result = debugger.analyze(prompt)

print("Prompt optimized!")
```

### Cost Monitoring
```python
result = debugger.analyze(prompt)

print(f"Cost per request: ${result.metrics.estimated_cost:.4f}")
print(f"Monthly cost (1000 req): ${result.metrics.estimated_cost * 1000:.2f}")

if result.metrics.unnecessary_tokens > 50:
    print(f"Potential savings: {result.metrics.unnecessary_tokens} tokens")
```

## Quick Tips

1. ğŸ¯ **Be specific**: Replace vague terms with exact numbers
2. ğŸ“ **Add examples**: Show desired output format
3. ğŸ”’ **Remove secrets**: Never include API keys or passwords
4. âš¡ **Be efficient**: Remove redundant phrases
5. âœ… **Define success**: Specify what "good" output looks like
6. ğŸ“Š **Add constraints**: Set clear boundaries
7. ğŸ¨ **Specify format**: JSON, markdown, code, etc.
8. ğŸ”„ **Iterate**: Use feedback to improve

## Links

- ğŸ“– Getting Started: `GETTING_STARTED.md`
- ğŸ“š Usage Guide: `docs/USAGE_GUIDE.md`
- ğŸ—ï¸ Architecture: `docs/ARCHITECTURE.md`
- ğŸ“‹ Summary: `PROJECT_SUMMARY.md`

---

**Quick Start**: `python demo.py` or `python -m src.cli.main analyze "Your prompt"`
